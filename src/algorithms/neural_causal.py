"""
Neural Causal Discovery with Attention Mechanisms
=================================================

Novel hybrid causal-neural architecture combining deep learning with causal constraints.
Implements attention-based variable selection and adversarial training for robust discovery.

Research Contributions:
- Attention mechanisms for causal variable selection  
- Spectral normalization for training stability
- DAG constraints in neural architecture
- Adversarial training for robustness

Target Venue: NeurIPS 2025
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from typing import Dict, List, Any, Optional, Tuple, Union
import warnings
from sklearn.preprocessing import StandardScaler
from dataclasses import dataclass
import logging

try:
    from .base import CausalDiscoveryModel, CausalResult
    from ..utils.validation import DataValidator
    from ..utils.metrics import CausalMetrics
except ImportError:
    from base import CausalDiscoveryModel, CausalResult
    from utils.validation import DataValidator
    from utils.metrics import CausalMetrics


@dataclass
class NeuralCausalConfig:
    """Configuration for Neural Causal Discovery."""
    hidden_dims: List[int] = None
    dag_constraint_weight: float = 1.0
    sparsity_penalty: float = 0.1
    attention_heads: int = 8
    attention_dropout: float = 0.1
    learning_rate: float = 0.001
    batch_size: int = 64
    max_epochs: int = 1000
    early_stopping_patience: int = 50
    adversarial_training: bool = True
    adversarial_weight: float = 0.1
    spectral_normalization: bool = True
    temperature: float = 0.5
    
    def __post_init__(self):
        if self.hidden_dims is None:
            self.hidden_dims = [128, 64, 32]


class SpectralNorm(nn.Module):
    """Spectral normalization for improved training stability."""
    
    def __init__(self, module, name='weight', power_iterations=1):
        super().__init__()
        self.module = module
        self.name = name
        self.power_iterations = power_iterations
        if not self._made_params():
            self._make_params()

    def _made_params(self):
        try:
            u = getattr(self.module, self.name + "_u")
            v = getattr(self.module, self.name + "_v")
            w = getattr(self.module, self.name + "_bar")
            return True
        except AttributeError:
            return False

    def _make_params(self):
        w = getattr(self.module, self.name)
        height = w.data.shape[0]
        width = w.view(height, -1).data.shape[1]
        
        u = nn.Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)
        v = nn.Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)
        w_bar = nn.Parameter(w.data)
        
        del self.module._parameters[self.name]
        self.module.register_parameter(self.name + "_u", u)
        self.module.register_parameter(self.name + "_v", v)
        self.module.register_parameter(self.name + "_bar", w_bar)

    def _update_u_v(self):
        u = getattr(self.module, self.name + "_u")
        v = getattr(self.module, self.name + "_v")
        w = getattr(self.module, self.name + "_bar")
        
        height = w.data.shape[0]
        for _ in range(self.power_iterations):
            v.data = F.normalize(torch.mv(torch.t(w.view(height, -1).data), u.data))
            u.data = F.normalize(torch.mv(w.view(height, -1).data, v.data))
        
        sigma = u.dot(w.view(height, -1).mv(v))
        setattr(self.module, self.name, w / sigma.expand_as(w))

    def forward(self, *args):
        self._update_u_v()
        return self.module.forward(*args)


class CausalAttention(nn.Module):
    """Multi-head attention for causal variable selection."""
    
    def __init__(self, 
                 input_dim: int, 
                 num_heads: int = 8,
                 dropout: float = 0.1,
                 temperature: float = 0.5):
        super().__init__()
        self.input_dim = input_dim
        self.num_heads = num_heads
        self.head_dim = input_dim // num_heads
        self.temperature = temperature
        
        assert input_dim % num_heads == 0, "input_dim must be divisible by num_heads"
        
        self.query = nn.Linear(input_dim, input_dim)
        self.key = nn.Linear(input_dim, input_dim)
        self.value = nn.Linear(input_dim, input_dim)
        self.output = nn.Linear(input_dim, input_dim)
        
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(input_dim)
        
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        batch_size, seq_len, _ = x.shape
        residual = x
        
        # Multi-head attention
        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Scaled dot-product attention with temperature scaling
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (np.sqrt(self.head_dim) * self.temperature)
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # Apply attention
        context = torch.matmul(attention_weights, V)
        context = context.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.input_dim
        )
        
        # Output projection and residual connection
        output = self.output(context)
        output = self.layer_norm(output + residual)
        
        # Return averaged attention weights for interpretability
        avg_attention = attention_weights.mean(dim=1)  # Average over heads
        
        return output, avg_attention


class DAGConstraintLayer(nn.Module):
    """Differentiable DAG constraint enforcement."""
    
    def __init__(self, num_variables: int):
        super().__init__()
        self.num_variables = num_variables
        # Learnable adjacency matrix with DAG constraints
        self.adjacency_logits = nn.Parameter(torch.randn(num_variables, num_variables))
        
    def forward(self) -> Tuple[torch.Tensor, torch.Tensor]:
        # Apply sigmoid to get probabilities
        adjacency = torch.sigmoid(self.adjacency_logits)
        
        # Zero out diagonal (no self-loops)
        mask = torch.eye(self.num_variables, device=adjacency.device)
        adjacency = adjacency * (1 - mask)
        
        # Compute DAG constraint (trace of matrix exponential should be num_variables)
        # Using approximation: tr(e^A) ‚âà tr(I + A + A¬≤/2 + A¬≥/6)
        A = adjacency
        A2 = torch.matmul(A, A)
        A3 = torch.matmul(A2, A)
        trace_exp_A = torch.trace(torch.eye(self.num_variables, device=A.device) + A + A2/2 + A3/6)
        dag_constraint = (trace_exp_A - self.num_variables) ** 2
        
        return adjacency, dag_constraint


class NeuralCausalEncoder(nn.Module):
    """Neural encoder with attention and spectral normalization."""
    
    def __init__(self, 
                 input_dim: int,
                 hidden_dims: List[int],
                 attention_heads: int = 8,
                 attention_dropout: float = 0.1,
                 spectral_norm: bool = True,
                 temperature: float = 0.5):
        super().__init__()
        
        self.attention = CausalAttention(
            input_dim, attention_heads, attention_dropout, temperature
        )
        
        # Build encoder layers
        layers = []
        dims = [input_dim] + hidden_dims
        
        for i in range(len(dims) - 1):
            linear = nn.Linear(dims[i], dims[i + 1])
            if spectral_norm:
                linear = SpectralNorm(linear)
            layers.extend([
                linear,
                nn.BatchNorm1d(dims[i + 1]),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])
        
        self.encoder = nn.Sequential(*layers)
        
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        # Add sequence dimension for attention (treat variables as sequence)
        x_seq = x.unsqueeze(1)  # (batch, 1, features)
        
        # Apply attention
        attended_x, attention_weights = self.attention(x_seq)
        attended_x = attended_x.squeeze(1)  # Remove sequence dimension
        
        # Apply encoder
        encoded = self.encoder(attended_x)
        
        return encoded, attention_weights.squeeze(1)


class AdversarialPerturbation(nn.Module):
    """Adversarial perturbation generator for robust training."""
    
    def __init__(self, input_dim: int, epsilon: float = 0.1):
        super().__init__()
        self.epsilon = epsilon
        self.perturbation_generator = nn.Sequential(
            nn.Linear(input_dim, input_dim // 2),
            nn.ReLU(),
            nn.Linear(input_dim // 2, input_dim),
            nn.Tanh()
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        perturbation = self.perturbation_generator(x)
        # Scale perturbation by epsilon
        perturbation = self.epsilon * perturbation
        return x + perturbation


class NeuralCausalDiscovery(CausalDiscoveryModel):
    """
    Neural Causal Discovery with Attention Mechanisms.
    
    A novel hybrid approach combining neural networks with causal constraints,
    featuring attention-based variable selection and adversarial training.
    """
    
    def __init__(self, config: Optional[NeuralCausalConfig] = None):
        super().__init__()
        self.config = config or NeuralCausalConfig()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.scaler = StandardScaler()
        self.model = None
        self.trained = False
        
    def _build_model(self, input_dim: int) -> nn.Module:
        """Build the neural causal discovery model."""
        
        class NeuralCausalModel(nn.Module):
            def __init__(self, config, input_dim):
                super().__init__()
                self.config = config
                self.input_dim = input_dim
                
                # Encoder with attention
                self.encoder = NeuralCausalEncoder(
                    input_dim=input_dim,
                    hidden_dims=config.hidden_dims,
                    attention_heads=config.attention_heads,
                    attention_dropout=config.attention_dropout,
                    spectral_norm=config.spectral_normalization,
                    temperature=config.temperature
                )
                
                # DAG constraint layer
                self.dag_constraint = DAGConstraintLayer(input_dim)
                
                # Adversarial perturbation (if enabled)
                if config.adversarial_training:
                    self.adversarial = AdversarialPerturbation(input_dim)
                
                # Causal structure decoder
                self.structure_decoder = nn.Sequential(
                    nn.Linear(config.hidden_dims[-1], input_dim * input_dim),
                    nn.Sigmoid()
                )
                
            def forward(self, x, adversarial_training=False):
                batch_size = x.shape[0]
                
                # Apply adversarial perturbation if in training mode
                if adversarial_training and hasattr(self, 'adversarial'):
                    x_adv = self.adversarial(x)
                else:
                    x_adv = x
                
                # Encode with attention
                encoded, attention_weights = self.encoder(x_adv)
                
                # Decode causal structure
                structure_logits = self.structure_decoder(encoded)
                structure = structure_logits.view(batch_size, self.input_dim, self.input_dim)
                
                # Get DAG constraint
                dag_adjacency, dag_loss = self.dag_constraint()
                
                return {
                    'causal_structure': structure,
                    'attention_weights': attention_weights,
                    'dag_adjacency': dag_adjacency,
                    'dag_constraint_loss': dag_loss,
                    'encoded_features': encoded
                }
        
        return NeuralCausalModel(self.config, input_dim)
    
    def _compute_loss(self, predictions, target_structure=None):
        """Compute multi-component loss function."""
        
        # Reconstruction loss (if target structure available)
        recon_loss = 0
        if target_structure is not None:
            target_tensor = torch.tensor(target_structure, dtype=torch.float32, device=self.device)
            recon_loss = F.mse_loss(predictions['causal_structure'], target_tensor)
        
        # DAG constraint loss
        dag_loss = predictions['dag_constraint_loss']
        
        # Sparsity penalty (encourage sparse causal structures)
        sparsity_loss = torch.mean(torch.abs(predictions['causal_structure']))
        
        # Total loss
        total_loss = (recon_loss + 
                     self.config.dag_constraint_weight * dag_loss + 
                     self.config.sparsity_penalty * sparsity_loss)
        
        return {
            'total_loss': total_loss,
            'reconstruction_loss': recon_loss,
            'dag_constraint_loss': dag_loss,
            'sparsity_loss': sparsity_loss
        }
    
    def fit(self, 
            data: np.ndarray, 
            target_structure: Optional[np.ndarray] = None,
            **kwargs) -> 'NeuralCausalDiscovery':
        """Fit the neural causal discovery model."""
        
        try:
            # Validate and preprocess data
            validator = DataValidator()
            data = validator.validate_data(data)
            
            # Standardize data
            data_scaled = self.scaler.fit_transform(data)
            n_samples, n_features = data_scaled.shape
            
            # Build model
            self.model = self._build_model(n_features).to(self.device)
            
            # Setup optimizer and data loader
            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.learning_rate)
            dataset = TensorDataset(torch.tensor(data_scaled, dtype=torch.float32))
            dataloader = DataLoader(dataset, batch_size=self.config.batch_size, shuffle=True)
            
            # Training loop with early stopping
            best_loss = float('inf')
            patience_counter = 0
            
            self.model.train()
            for epoch in range(self.config.max_epochs):
                epoch_losses = []
                
                for batch in dataloader:
                    batch_data = batch[0].to(self.device)
                    
                    # Forward pass
                    predictions = self.model(batch_data, 
                                           adversarial_training=self.config.adversarial_training)
                    
                    # Compute loss
                    losses = self._compute_loss(predictions, target_structure)
                    
                    # Backward pass
                    optimizer.zero_grad()
                    losses['total_loss'].backward()
                    
                    # Gradient clipping for stability
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    
                    optimizer.step()
                    epoch_losses.append(losses['total_loss'].item())
                
                # Early stopping check
                avg_loss = np.mean(epoch_losses)
                if avg_loss < best_loss:
                    best_loss = avg_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                
                if patience_counter >= self.config.early_stopping_patience:
                    logging.info(f"Early stopping at epoch {epoch}")
                    break
                
                if epoch % 100 == 0:
                    logging.info(f"Epoch {epoch}, Loss: {avg_loss:.6f}")
            
            self._fitted_data = data
            self.trained = True
            return self
            
        except Exception as e:
            logging.error(f"Error in neural causal discovery training: {e}")
            raise
    
    def discover_causal_structure(self, data: np.ndarray, **kwargs) -> CausalResult:
        """Discover causal structure using the trained neural model."""
        
        if not self.trained:
            # Fit the model if not already trained
            self.fit(data, **kwargs)
        
        try:
            # Preprocess data
            data_scaled = self.scaler.transform(data)
            data_tensor = torch.tensor(data_scaled, dtype=torch.float32).to(self.device)
            
            # Get predictions
            self.model.eval()
            with torch.no_grad():
                predictions = self.model(data_tensor)
            
            # Extract causal structure (average over samples)
            causal_structure = predictions['causal_structure'].mean(dim=0).cpu().numpy()
            attention_weights = predictions['attention_weights'].mean(dim=0).cpu().numpy()
            dag_adjacency = predictions['dag_adjacency'].cpu().numpy()
            
            # Apply threshold for binary adjacency matrix
            threshold = 0.5
            binary_structure = (causal_structure > threshold).astype(int)
            
            # Compute confidence scores and metrics
            confidence_scores = np.abs(causal_structure - 0.5) * 2  # Distance from uncertainty
            
            # Create result with comprehensive information
            result = CausalResult(
                causal_matrix=binary_structure,
                confidence_scores=confidence_scores,
                method_name="Neural Causal Discovery with Attention",
                metadata={
                    'continuous_structure': causal_structure,
                    'attention_weights': attention_weights,
                    'dag_adjacency': dag_adjacency,
                    'model_config': self.config.__dict__,
                    'n_parameters': sum(p.numel() for p in self.model.parameters()),
                    'sparsity': np.mean(binary_structure == 0),
                    'interpretability_score': np.mean(attention_weights)
                }
            )
            
            return result
            
        except Exception as e:
            logging.error(f"Error in neural causal structure discovery: {e}")
            raise
    
    def discover(self, data: Optional[pd.DataFrame] = None) -> CausalResult:
        """Discover causal relationships."""
        if data is None:
            if not hasattr(self, '_fitted_data'):
                raise ValueError("No data provided and model has no fitted data")
            data = self._fitted_data
        
        # Convert DataFrame to numpy array if needed
        if isinstance(data, pd.DataFrame):
            data = data.values
        
        return self.discover_causal_structure(data)

    def explain_discovery(self, result: CausalResult) -> Dict[str, Any]:
        """Provide detailed explanation of the neural causal discovery."""
        
        explanation = {
            'method_description': (
                "Neural Causal Discovery with Attention Mechanisms uses a hybrid approach "
                "combining deep learning with causal constraints. The model employs "
                "multi-head attention for variable selection and enforces DAG constraints "
                "through differentiable optimization."
            ),
            'attention_analysis': {
                'most_important_variables': np.argsort(result.metadata['attention_weights'])[::-1][:5],
                'attention_distribution': result.metadata['attention_weights'],
                'variable_importance_scores': result.metadata['attention_weights']
            },
            'causal_structure_analysis': {
                'sparsity_level': result.metadata['sparsity'],
                'strongest_relationships': self._get_strongest_relationships(result.causal_matrix),
                'dag_validity': self._check_dag_validity(result.causal_matrix)
            },
            'model_complexity': {
                'n_parameters': result.metadata['n_parameters'],
                'interpretability_score': result.metadata['interpretability_score']
            },
            'novel_contributions': [
                "Attention-based variable selection for causal discovery",
                "Spectral normalization for training stability", 
                "Adversarial training for robust causal inference",
                "Differentiable DAG constraint enforcement"
            ]
        }
        
        return explanation
    
    def _get_strongest_relationships(self, causal_matrix: np.ndarray) -> List[Tuple[int, int, float]]:
        """Get the strongest causal relationships."""
        relationships = []
        for i in range(causal_matrix.shape[0]):
            for j in range(causal_matrix.shape[1]):
                if i != j and causal_matrix[i, j] > 0:
                    relationships.append((i, j, causal_matrix[i, j]))
        
        return sorted(relationships, key=lambda x: x[2], reverse=True)[:10]
    
    def _check_dag_validity(self, causal_matrix: np.ndarray) -> Dict[str, Any]:
        """Check if the discovered structure is a valid DAG."""
        try:
            # Check for cycles using matrix powers
            n = causal_matrix.shape[0]
            matrix_power = causal_matrix.copy()
            
            for k in range(2, n + 1):
                matrix_power = np.matmul(matrix_power, causal_matrix)
                if np.trace(matrix_power) > 0:
                    return {'is_dag': False, 'cycle_detected': True, 'reason': f'Cycle detected at power {k}'}
            
            return {'is_dag': True, 'cycle_detected': False, 'reason': 'Valid DAG structure'}
            
        except Exception as e:
            return {'is_dag': False, 'cycle_detected': None, 'reason': f'Error checking DAG: {e}'}


# Example usage and demonstration
def demonstrate_neural_causal_discovery():
    """Demonstrate the neural causal discovery algorithm."""
    
    print("üß† Neural Causal Discovery with Attention - Research Demo")
    print("=" * 60)
    
    # Generate synthetic data with known causal structure
    np.random.seed(42)
    n_samples, n_features = 1000, 5
    
    # Create true causal structure (DAG)
    true_structure = np.array([
        [0, 1, 0, 1, 0],  # X0 -> X1, X3
        [0, 0, 1, 0, 0],  # X1 -> X2
        [0, 0, 0, 1, 1],  # X2 -> X3, X4
        [0, 0, 0, 0, 1],  # X3 -> X4
        [0, 0, 0, 0, 0]   # X4 (no outgoing edges)
    ])
    
    # Generate data following the causal structure
    data = np.random.randn(n_samples, n_features)
    data[:, 1] += 0.5 * data[:, 0] + 0.1 * np.random.randn(n_samples)  # X0 -> X1
    data[:, 2] += 0.7 * data[:, 1] + 0.1 * np.random.randn(n_samples)  # X1 -> X2
    data[:, 3] += 0.6 * data[:, 0] + 0.4 * data[:, 2] + 0.1 * np.random.randn(n_samples)  # X0, X2 -> X3
    data[:, 4] += 0.8 * data[:, 2] + 0.3 * data[:, 3] + 0.1 * np.random.randn(n_samples)  # X2, X3 -> X4
    
    # Configure neural causal discovery
    config = NeuralCausalConfig(
        hidden_dims=[64, 32, 16],
        dag_constraint_weight=2.0,
        sparsity_penalty=0.2,
        attention_heads=4,
        learning_rate=0.001,
        batch_size=32,
        max_epochs=500,
        adversarial_training=True
    )
    
    # Initialize and train model
    neural_discovery = NeuralCausalDiscovery(config)
    print("\nüèãÔ∏è Training Neural Causal Discovery Model...")
    
    # Discover causal structure
    result = neural_discovery.discover_causal_structure(data, target_structure=true_structure)
    
    print("\nüìä Results:")
    print(f"True Structure:\n{true_structure}")
    print(f"Discovered Structure:\n{result.causal_matrix}")
    
    # Compute accuracy metrics
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    
    true_flat = true_structure.flatten()
    pred_flat = result.causal_matrix.flatten()
    
    accuracy = accuracy_score(true_flat, pred_flat)
    precision = precision_score(true_flat, pred_flat, zero_division=0)
    recall = recall_score(true_flat, pred_flat, zero_division=0)
    f1 = f1_score(true_flat, pred_flat, zero_division=0)
    
    print(f"\nüéØ Performance Metrics:")
    print(f"Accuracy: {accuracy:.3f}")
    print(f"Precision: {precision:.3f}")
    print(f"Recall: {recall:.3f}")
    print(f"F1-Score: {f1:.3f}")
    print(f"Sparsity: {result.metadata['sparsity']:.3f}")
    
    # Get detailed explanation
    explanation = neural_discovery.explain_discovery(result)
    print(f"\nüîç Analysis:")
    print(f"Model Parameters: {explanation['model_complexity']['n_parameters']:,}")
    print(f"Interpretability Score: {explanation['model_complexity']['interpretability_score']:.3f}")
    print(f"DAG Valid: {explanation['causal_structure_analysis']['dag_validity']['is_dag']}")
    
    print(f"\n‚ú® Novel Contributions:")
    for contribution in explanation['novel_contributions']:
        print(f"  ‚Ä¢ {contribution}")
    
    return neural_discovery, result, explanation


if __name__ == "__main__":
    demonstrate_neural_causal_discovery()