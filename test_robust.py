#!/usr/bin/env python3
"""Test script to verify robust causal discovery functionality."""

import sys
import os
import pandas as pd
import numpy as np

# Add current directory to path for package import
sys.path.insert(0, os.path.dirname(__file__))

try:
    from causal_discovery_toolkit import CausalDiscoveryModel
    from causal_discovery_toolkit.algorithms.robust import RobustSimpleLinearCausalModel
    from causal_discovery_toolkit import DataProcessor, CausalMetrics
    
    print("âœ… Robust modules imported successfully")
    
    # Test with good data
    print("\nğŸ§ª Testing with clean data...")
    np.random.seed(42)
    clean_data = pd.DataFrame({
        'X1': np.random.randn(100),
        'X2': np.random.randn(100), 
        'X3': np.random.randn(100),
        'X4': np.random.randn(100)
    })
    
    # Add causal relationships
    clean_data['X2'] = 0.5 * clean_data['X1'] + 0.3 * np.random.randn(100)
    clean_data['X3'] = 0.7 * clean_data['X2'] + 0.2 * np.random.randn(100)
    
    model = RobustSimpleLinearCausalModel(threshold=0.3)
    model.fit(clean_data)
    result = model.discover()
    
    print(f"âœ… Clean data test passed - {result.metadata['n_edges']} edges found")
    
    # Test model health
    health = model.validate_health()
    print(f"âœ… Model health check: {'âœ… Healthy' if health['healthy'] else 'âŒ Unhealthy'}")
    
    # Test with problematic data
    print("\nğŸ§ª Testing with missing data...")
    dirty_data = clean_data.copy()
    # Add some missing values
    dirty_data.loc[10:20, 'X1'] = np.nan
    dirty_data.loc[30:35, 'X2'] = np.nan
    
    model_robust = RobustSimpleLinearCausalModel(
        threshold=0.3, 
        handle_missing='drop',
        validate_inputs=True
    )
    model_robust.fit(dirty_data)
    result_robust = model_robust.discover()
    
    print(f"âœ… Missing data test passed - {result_robust.metadata['n_edges']} edges found")
    print(f"  Original shape: {dirty_data.shape} -> Final shape: {result_robust.metadata['processed_shape']}")
    
    # Test with very small data (should fail gracefully)
    print("\nğŸ§ª Testing with insufficient data...")
    small_data = clean_data.head(5)  # Too few samples
    
    try:
        model_small = RobustSimpleLinearCausalModel(min_samples=10)
        model_small.fit(small_data)
        print("âŒ Should have failed with small data")
    except Exception as e:
        print(f"âœ… Correctly rejected small data: {type(e).__name__}")
    
    # Test with non-numeric data
    print("\nğŸ§ª Testing with non-numeric data...")
    mixed_data = clean_data.copy()
    mixed_data['category'] = ['A', 'B'] * 50
    
    model_mixed = RobustSimpleLinearCausalModel(validate_inputs=False)  # Skip validation to test preprocessing
    model_mixed.fit(mixed_data)  # Should automatically filter to numeric columns
    result_mixed = model_mixed.discover()
    
    print(f"âœ… Mixed data test passed - filtered to numeric columns")
    print(f"  Variables: {result_mixed.metadata['variable_names']}")
    
    # Test error recovery with circuit breaker
    print("\nğŸ§ª Testing circuit breaker...")
    model_cb = RobustSimpleLinearCausalModel()
    model_cb.fit(clean_data)
    
    # Force some failures to test circuit breaker
    try:
        # This should work normally
        result_cb = model_cb.discover()
        print(f"âœ… Normal discovery succeeded")
        
        # Get model info
        info = model_cb.get_model_info()
        print(f"âœ… Model info retrieved: {info['state']['is_fitted']}")
        
    except Exception as e:
        print(f"Circuit breaker test issue: {e}")
    
    # Test different correlation methods
    print("\nğŸ§ª Testing different correlation methods...")
    methods = ['pearson', 'spearman', 'kendall']
    
    for method in methods:
        try:
            model_corr = RobustSimpleLinearCausalModel(
                correlation_method=method,
                threshold=0.3
            )
            model_corr.fit(clean_data)
            result_corr = model_corr.discover()
            print(f"âœ… {method.capitalize()} correlation: {result_corr.metadata['n_edges']} edges")
        except Exception as e:
            print(f"âŒ {method.capitalize()} correlation failed: {e}")
    
    print("\nğŸ‰ ALL ROBUST TESTS PASSED - Generation 2 Complete!")
    
    # Show comprehensive metrics
    print("\nğŸ“Š Final Performance Summary:")
    print(f"  â€¢ Method: {result.method_used}")
    print(f"  â€¢ Variables: {result.metadata['n_variables']}")
    print(f"  â€¢ Edges found: {result.metadata['n_edges']}")
    print(f"  â€¢ Sparsity: {result.metadata['sparsity']:.3f}")
    print(f"  â€¢ Max confidence: {result.metadata['max_confidence']:.3f}")
    print(f"  â€¢ Mean confidence: {result.metadata['mean_confidence']:.3f}")

except ImportError as e:
    print(f"âŒ Import error: {e}")
    sys.exit(1)
except Exception as e:
    print(f"âŒ Runtime error: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)